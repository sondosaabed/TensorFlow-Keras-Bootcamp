{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX0CLHEN3536-2023-01-01\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Beat CNN with Vision Transformers for Image Classification**\n",
    "\n",
    "Estimated time needed: **60** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image classification refers to the task of assigning a label or a category to an input image based on its visual content. It involves training a model to recognize and differentiate between different objects, scenes, or patterns within images.\n",
    "\n",
    "Vision Transformers (ViTs) are an exciting development in the field of computer vision, leveraging the Transformer architecture initially designed for natural language processing. The introduction of Transformers revolutionized NLP by effectively capturing long-range dependencies and achieving exceptional performance on tasks like machine translation and language understanding.\n",
    "Now, this transformative architecture has been successfully applied to image classification tasks, yielding promising outcomes that often surpass the capabilities of traditional Convolutional Neural Networks (CNNs). This recent advancement in image classification using ViTs has created a significant buzz in the field. It is essential to familiarize yourself with the concept and knowledge surrounding ViTs in order to fully exploit their potential and stay up to date with the latest developments in this rapidly evolving domain.\n",
    "The Vision Transformer is a model  treats images as sequences of smaller patches, like a sentence we assume that part of the images are related. We show in the image by <a href=https://github.com/lucidrains/vit-pytorch/blob/main/images/vit.gif>Phil Wang</a> , an image is divided into 9 patches. Each patch is considered a \"word\" or \"token\" and is transformed into a feature space. By adding positional encodings and a classification token, we can use a Transformer model to train on this sequence for image classification. Below is a GIF that visualizes the architecture.\n",
    "\n",
    "\n",
    "<img src=https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX0CLHEN/vit.gif />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Table of Contents__\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"#Objectives\">Objectives</a></li>\n",
    "    <li>\n",
    "        <a href=\"#Setup\">Setup</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Installing-Required-Libraries\">Installing Required Libraries</a></li>\n",
    "            <li><a href=\"#Importing-Required-Libraries\">Importing Required Libraries</a></li>\n",
    "            <li><a href=\"#Defining-Helper-Functions\">Defining Helper Functions</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Background---Vision-Transformers\">Background - Vision Transformers</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#What-is-self-attention?\">What is self-attention?</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><a href=\"#Loading-MNIST-dataset\">Loading MNIST dataset</a></li>\n",
    "    <li><a href=\"#Images-as-Sequences\">Images as Sequences</a></li>\n",
    "    <li><a href=\"#Constructing-Vision-Transformer-Model\">Constructing Vision Transformer Model</a></li>\n",
    "    <li><a href=\"#Loading-Pre-trained-model\">Loading Pre-trained model</a></li>\n",
    "    <li><a href=\"#Output-for-the-model\">Output for the model</a></li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    "\n",
    "- PyTorch: In this guided project, you will work with the PyTorch library to build and train a vision transformer specifically for image classification tasks. By leveraging the power of PyTorch, you will develop an efficient and accurate model to classify images effectively.\n",
    "- Vision Transformers: You will explore the concept of vision transformers to enhance the efficiency and accuracy of your image classification system. Additionally, you will learn about their implementation to further refine the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, we will be using the following libraries:\n",
    "\n",
    "*   [numpy](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for mathematical and array operations.\n",
    "*   [sklearn](https://scikit-learn.org/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for machine learning and machine-learning-pipeline related functions.\n",
    "*   [torchvision](https://pytorch.org/vision/stable/index.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX05WUEN3552-2023-01-01) for deep learning and neural network-related functions.\n",
    "*   [torch](https://pytorch.org/docs/stable/torch.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX05WUEN3552-2023-01-01) for multi-dimensional tensors and mathematical operations over tensors.\n",
    "*   [matplotlib](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for additional plotting tools.\n",
    "*   [PIL](https://pillow.readthedocs.io/en/stable/handbook/concepts.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX05WUEN3552-2023-01-01) for loading images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Required Libraries\n",
    "\n",
    "The following required libraries are pre-installed in the Skills Network Labs environment. However, if you run this notebook commands in a different Jupyter environment (e.g. Watson Studio or Ananconda), you will need to install these libraries by removing the `#` sign before `!pip` in the code cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n",
    "# !pip install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 scikit-learn==0.20.1\n",
    "# - Update a specific package\n",
    "# !pip install pmdarima -U\n",
    "# - Update a package to specific version\n",
    "# !pip install --upgrade pmdarima==2.0.2\n",
    "# Note: If your environment doesn't support \"!pip install\", use \"!mamba install\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries\n",
    "\n",
    "_We recommend you import all required libraries in one place (here):_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn.manifold import TSNE\n",
    "from tqdm import tqdm\n",
    "\n",
    "import skillsnetwork\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ExponentialLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** - Upgrade scikit-learn version by using the below command and restart the kernel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Helper Functions\n",
    "\n",
    "Plot randomly selected samples from a dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(dataset, size_even=6, classes=None):\n",
    "    \n",
    "    size_even = 6  # Default number of samples to plot\n",
    "    dataset = train_set  # Default dataset to use (assuming 'train_set')\n",
    "    fig, axs = plt.subplots(2, int(size_even // 2))  # Create subplots for the samples\n",
    "\n",
    "    if classes is None:\n",
    "        classes = tuple([n for n in range(len(dataset))])  # Default class labels\n",
    "\n",
    "    n_samples = len(dataset)  # Total number of samples in the dataset\n",
    "\n",
    "    # Randomly select samples\n",
    "    samples = np.random.randint(0, high=n_samples - 1, size=int(size_even))\n",
    "\n",
    "    row = 0\n",
    "    col = 0\n",
    "    for n, sample in enumerate(samples):\n",
    "        img = dataset[sample][0]  # Get the image from the dataset\n",
    "        label = \"y={}\".format(classes[int(dataset[sample][1])])  # Get the label for the image\n",
    "\n",
    "        col = n\n",
    "\n",
    "        # Determine the row and column index for the current sample\n",
    "        if n > size_even / 2 - 1:\n",
    "            row = 1\n",
    "            col = n - int(size_even / 2)\n",
    "\n",
    "        # Display the image and label in the corresponding subplot\n",
    "        axs[row, col].imshow(make_grid(img, normalize=True).permute(1, 2, 0))\n",
    "        axs[row, col].set_title(label)\n",
    "        axs[row, col].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Plot a grid of image patches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_patchs(patchs, seq=False):\n",
    "\n",
    "    if patchs.shape[-3] == 1:\n",
    "        patchs = patchs.repeat(1, 1, 3, 1, 1)  # Add 3 channel dimension if image is grayscale\n",
    "\n",
    "    N_patchs = patchs.shape[1]  # Number of image patches\n",
    "\n",
    "    plot_patchs = make_grid(patchs, normalize=True).permute(0, 2, 3, 1)  # Arrange patches for plotting\n",
    "\n",
    "    N_rows = int(np.sqrt(N_patchs))  # Number of rows for the grid plot\n",
    "\n",
    "    # Create subplots for the image patches\n",
    "    if seq:\n",
    "        fig, ax = plt.subplots(1, N_rows * N_rows, sharex='col', sharey='row', figsize=(25, 5))\n",
    "        fig.suptitle(\"Image as Sequence\")\n",
    "    else:\n",
    "        fig, ax = plt.subplots(N_rows, N_rows, sharex='col', sharey='row')\n",
    "        fig.suptitle(\"Image\")\n",
    "\n",
    "    i, j = 0, 0\n",
    "    for n in range(N_patchs):\n",
    "        if seq:\n",
    "            ax[n].imshow(plot_patchs[n])\n",
    "            ax[n].set_xlabel(str(n + 1))\n",
    "            ax[n].axes.xaxis.set_ticklabels([])\n",
    "            ax[n].axes.yaxis.set_ticklabels([])\n",
    "        else:\n",
    "            if n % N_rows == 0 and n != 0:\n",
    "                i += 1\n",
    "                j = 0\n",
    "\n",
    "            ax[i, j].imshow(plot_patchs[n])\n",
    "            ax[i, j].set_ylabel(str(n + 1))\n",
    "            ax[i, j].axes.xaxis.set_ticklabels([])\n",
    "            ax[i, j].axes.yaxis.set_ticklabels([])\n",
    "            j += 1\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code implements a function for plotting t-SNE visualization of patches and their labels, with the option to label each point and control the frequency of labeling for the sequence index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_patchs_lables(X, y, label=True, p_show=0.5):\n",
    "    '''\n",
    "    The code implements a function for plotting t-SNE visualization of patches and their labels, with the option to label each point and control\n",
    "    the frequency of labeling for the sequence index.\n",
    "\n",
    "    X: This is a tensor representing the data to be plotted. It should have the shape (sample, patch, dim), where sample is the number of samples, patch is the size of each patch or sequence length, and dim is the number of features for each patch.\n",
    "    y: This is a tensor representing the labels for each sample. It should have the shape (sample, 1).\n",
    "    label: This is a boolean value that controls whether to label each data point in the plot. If label is set to True, each data point in the plot will be labeled.\n",
    "    p_show: This is a float value between 0 and 1 that controls the frequency of labeling for the sequence index. It specifies the probability of showing a label for each data point. For example, if p_show is set to 0.5, then approximately half of the data points will be labeled.\n",
    "    '''\n",
    "\n",
    "    # Detach X from its computational graph if it's not a leaf node\n",
    "    if not (X.is_leaf):\n",
    "        X = X.detach()\n",
    "\n",
    "    # Get the shape of the input tensor X\n",
    "    if X.shape[0] == 49:\n",
    "        sample = X.shape[1]\n",
    "        patch = X.shape[0]  # Patch size or sequence length\n",
    "        dim = X.shape[2]\n",
    "    if X.shape[0] == 1000:\n",
    "        sample = X.shape[0]\n",
    "        patch = X.shape[1]\n",
    "        dim = X.shape[2]\n",
    "\n",
    "    # Reshape X and convert it to a numpy array\n",
    "    X = X.reshape(sample * patch, dim).numpy()\n",
    "\n",
    "    # Flatten the y tensor and convert it to a numpy array to label each sequence\n",
    "    colors = y.repeat(patch, 1).T.flatten().numpy()\n",
    "\n",
    "    # Perform t-SNE on the X data\n",
    "    X_ = TSNE(n_components=2, learning_rate='auto', init='random').fit_transform(X)\n",
    "\n",
    "    # Create a scatter plot of the t-SNE transformed X data with each class labeled via color\n",
    "    fig, ax = plt.subplots()\n",
    "    for color in np.unique(colors):\n",
    "        temp = colors == color\n",
    "        ax.scatter(X_[temp, 0], X_[temp, 1], label=color)\n",
    "\n",
    "    # If the label argument is set to True, add labels to the scatter plot\n",
    "    if label:\n",
    "        for i, x in enumerate(X_):\n",
    "            # Label plots with a probability of p_show\n",
    "            if np.random.binomial(1, p_show, 1).item() == 1:\n",
    "                ax.annotate(str(i % patch), (x[0], x[1]))\n",
    "\n",
    "    # Add a legend to the scatter plot\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert an image into a set of vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_patch(x, patch_size, flatten_channels=True):\n",
    "\n",
    "    B, C, H, W = x.shape\n",
    "    x = x.reshape(B, C, H//patch_size, patch_size, W//patch_size, patch_size)\n",
    "    x = x.permute(0, 2, 4, 1, 3, 5) # [B, H', W', C, p_H, p_W]\n",
    "    x = x.flatten(1,2)              # [B, H'*W', C, p_H, p_W]\n",
    "    if flatten_channels:\n",
    "        x = x.flatten(2,4)          # [B, H'*W', C*p_H*p_W]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background - Vision Transformers\n",
    "\n",
    "Vision Transformers are a groundbreaking deep learning architecture that brings the power of the Transformer model, initially developed for natural language processing (NLP), into computer vision. They have completely transformed how image recognition is approached by substituting the conventional convolutional neural networks (CNNs) with **self-attention mechanisms**. This upgrade enables Vision Transformers to process visual data in a highly efficient and parallelized manner. Notably, Vision Transformers have outperformed traditional CNN-based methods, setting new standards in object detection, image classification, and segmentation tasks. Their exceptional performance on various benchmarks highlights their effectiveness and potential in the field of computer vision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is self-attention?\n",
    "\n",
    "In Vision Transformers, the entire image is divided into smaller sections called patches. These patches are similar to pieces of a puzzle that come together to form the complete picture. What's fascinating is that each patch has the ability to \"look at\" and consider all the other patches in the image. It pays attention to their features and tries to comprehend how they are connected to each other. This global attention mechanism allows the model to analyze the image as a whole and capture the relationships between distant regions.\n",
    "\n",
    "Vision Transformers begin by focusing on local patches and gradually expand their understanding to encompass the broader context. This unique capability of capturing both local and global relationships is what sets them apart. They can effectively learn from various parts of the image and gain insight into how these parts fit together.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading MNIST dataset\n",
    "The MNIST dataset is an extensively utilized benchmark in machine learning and computer vision and is  is a widely used benchmark dataset for image classification tasks. It comprises a vast collection of 70,000 handwritten digits ranging from 0 to 9. Each digit is represented as a grayscale image with dimensions of 28x28 pixels. The MNIST dataset acts as a standard for evaluating and comparing algorithms and models for tasks such as image classification. Its simplicity and accessibility have made it a go-to option for beginners to delve into and grasp various machine learning techniques. For more information on the MNIST dataset, you can visit [here](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX0CLHEN3536-2023-01-01#torchvision.datasets.MNIST). We would like to classify the digit given the image of the digit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Normalization*** refers to the process of adjusting the intensity values of image pixels to a specific range. One common approach is to scale the pixel values to fall within the range of [0, 1]. Another approach is standardization, which brings the pixel values to a range of [-1, 1].\n",
    "\n",
    "**Why do we use Normalization?**\n",
    "\n",
    "***Normalization*** serves multiple purposes in data preprocessing for machine learning tasks. It helps prevent certain features from dominating the training process and can aid in reducing computational costs. By bringing the pixel values to a similar scale, normalization facilitates the convergence of models and contributes to more accurate results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_TRAIN = 100\n",
    "BATCH_SIZE_TEST = 1000\n",
    "\n",
    "transform_mnist = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "train_set = torchvision.datasets.MNIST(root='.',train=True, download=True,\n",
    "                                       transform=transform_mnist)\n",
    "\n",
    "test_set = torchvision.datasets.MNIST(root='.',train=False, download=True,\n",
    "                                      transform=transform_mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the torchvision library, the **`DataLoader`** class is used to create an iterable over a dataset. It provides various features to efficiently load and preprocess data for training or inference in PyTorch.\n",
    "\n",
    "It can be used to create batches, shuffle these batches, multi-threading and data prefetching. Read more about [**DataLoader**](https://pytorch.org/vision/stable/datasets.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX05WUEN3552-2023-01-01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE_TRAIN, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE_TEST, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images as Sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line creates an iterator **`it`** from the **`test_loader`** data loader. The iterator allows us to iterate over the test dataset in batches, enabling us to access and process the data in a sequential manner.\n",
    "\n",
    "The second line retrieves the next batch of data from the iterator using the **`next()`** function. By accessing the element at index 0, it retrieves the input images from the batch. In this case, it fetches the input image from the first batch of the test dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it=iter(test_loader)\n",
    "image=next(it)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, **```image```** is assumed to be a tensor, and **```B, C, H```** and **```W```** represent the batch size, number of channels, height, and width respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, C, H, W = image.shape\n",
    "print(\"batch size: {}, number of channels: {}, height: {},  width: {}\".format( B, C, H, W ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image itself represents the last two dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image[0][0].numpy(),cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To split the image into patches, we will use a patch size of 4. The number of batches can be determined by dividing the size of the image by the patch size and squaring the result, assuming that the patches form a square shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size=4\n",
    "\n",
    "n_patches=(H/patch_size)* (W/patch_size)\n",
    "n_patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the option to convert the image into a set of patches. In this case, when **`flatten_channels`** is set to **`False`**, the patches are arranged in an image grid format instead of flattening the channels. This arrangement enables us to visualize the patches as a grid and use them as a feature vector for further analysis or application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches=img_to_patch(torch.unsqueeze(test_set[1][0],0), patch_size, flatten_channels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the batch dimension, which represents the number of images in a batch. Additionally, we have the number of patches, which corresponds to the patches extracted from each image. For color images, we also have the color channel dimension, representing the RGB channels. Each image patch contains the pixel information for a specific region of the image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the image as a patch, preserving its original shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image_patchs(patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see the image as a sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image_patchs(patches, seq=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When compared to the original images, the patch lists present a greater challenge for object recognition. The input format provided to the Transformer for image classification consists of these individual patches. In this format, the model needs to learn how to effectively combine these patches to recognize the objects within the image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 - Use the function **```img_to_patch```** convert the image to sequence set **```flatten_channels=True```** and find the shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for a Hint</summary>\n",
    "```python\n",
    "patchs=img_to_patch(torch.unsqueeze(test_set[1][0],0), patch_size, flatten_channels=True)\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 - What does the Dimensions of each Patch mean?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for a Hint</summary>\n",
    "```\n",
    "\n",
    "Batch Dimension: The first dimension represents the batch size, indicating the number of patches or images within a batch. It allows for processing multiple patches simultaneously.\n",
    "\n",
    "Sequence Dimension: The second dimension represents the sequence length or patch size. It signifies the number of elements or pixels within each patch. In other words, it captures the spatial extent of the patches.\n",
    "\n",
    "Feature Dimension: The final dimension corresponds to the features or channels within the patches. For example, in the case of color images, this dimension represents the RGB channels. In the case of grayscale images, this dimension may represent the intensity or other image features. It captures additional information or characteristics of the patches.\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing Vision Transformer Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now construct a Transformer model specifically designed for image-related tasks, utilizing the **`nn.MultiheadAttention`** module provided by PyTorch.\n",
    "\n",
    "Additionally, we adopt the Pre-Layer Normalization version of Transformer blocks, proposed by Ruibin Xiong et al. in 2020. This variant places Layer Normalization as the first layer within the residual blocks, deviating from the traditional approach of applying it between residual blocks. This reorganization improves gradient flow and eliminates the need for a warm-up stage.\n",
    "\n",
    "The AttentionBlock class comprises several components:\n",
    "\n",
    "- **`layer_norm_1`**: nn.LayerNorm (PyTorch Documentation)\n",
    "- **`attn`**: nn.MultiheadAttention (PyTorch Documentation)\n",
    "- **`layer_norm_2`**: nn.LayerNorm (PyTorch Documentation)\n",
    "- **`linear`**: nn.Sequential (PyTorch Documentation)\n",
    "\n",
    "These components collaborate to form the attention block within the Transformer model, functioning similarly to a convolutional layer in a CNN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX0CLHEN/transformer_Layer%20.png\" width=\"30%\" alt=\"cheques image\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will be using multiple Attention Blocks in our Transformer model, we will create a dedicated class to conveniently configure each block. This class allows us to specify the dimensions and number of attention heads for individual blocks.\n",
    "\n",
    "The class supports the following parameters:\n",
    "\n",
    "- **`embed_dim`**: This parameter determines the size of the input and attention feature vectors.\n",
    "- **`hidden_dim`**: It defines the dimensionality of the hidden layer in the feed-forward network. Typically, it is chosen to be 2-4 times larger than embed_dim.\n",
    "- **`num_heads`**: We can select the number of attention heads to be utilized in the Multi-Head Attention block. This facilitates parallel processing and captures various attention patterns.\n",
    "By instantiating this class with different parameter values, we can easily customize each Attention Block within our Transformer model to suit our specific requirements.\n",
    "\n",
    "This approach offers flexibility for experimentation with various configurations, aiding in the identification of optimal settings for our task.\n",
    "\n",
    "Feel free to modify these parameters and explore the possibilities to achieve the best performance for your application!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, hidden_dim, num_heads, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            embed_dim - Dimensionality of input and attention feature vectors\n",
    "            hidden_dim - Dimensionality of hidden layer in feed-forward network\n",
    "                         (usually 2-4x larger than embed_dim)\n",
    "            num_heads - Number of heads to use in the Multi-Head Attention block\n",
    "            dropout - Amount of dropout to apply in the feed-forward network\n",
    "            #source:https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial15/Vision_Transformer.html\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm_1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads,\n",
    "                                          dropout=dropout)\n",
    "        self.layer_norm_2 = nn.LayerNorm(embed_dim)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        inp_x = self.layer_norm_1(x)\n",
    "        x = x + self.attn(inp_x, inp_x, inp_x)[0]\n",
    "        x = x + self.linear(self.layer_norm_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VisionTransformer is utilized for image classification and follows a set of steps to process the input image:\n",
    "\n",
    "1. **Preprocessing**: The img_to_patch function divides the input image into patches.\n",
    "2. **Input Encoding**: The patches pass through an input layer, generating feature vectors.\n",
    "3. **Positional Encoding**: The feature vectors incorporate positional embeddings and a CLS token, capturing spatial information.\n",
    "4. **Transformer Blocks**: Multiple Transformer blocks process the feature vectors and positional embeddings, employing self-attention mechanisms to capture patch dependencies and extract meaningful representations.\n",
    "5. **Classification Prediction**: The output from the Transformer blocks enters an MLP head, responsible for the final classification prediction.\n",
    "\n",
    "The VisionTransformer class encapsulates the necessary layers and parameters to construct and execute this model.\n",
    "\n",
    "Please note that the code may reference additional functions and classes, such as AttentionBlock and img_to_patch, which handle specific operations within the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, hidden_dim, num_channels, num_heads, num_layers, num_classes, patch_size, num_patches, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            embed_dim - Dimensionality of the input feature vectors to the Transformer\n",
    "            hidden_dim - Dimensionality of the hidden layer in the feed-forward networks\n",
    "                         within the Transformer\n",
    "            num_channels - Number of channels of the input (3 for RGB)\n",
    "            num_heads - Number of heads to use in the Multi-Head Attention block\n",
    "            num_layers - Number of layers to use in the Transformer\n",
    "            num_classes - Number of classes to predict\n",
    "            patch_size - Number of pixels that the patches have per dimension\n",
    "            num_patches - Maximum number of patches an image can have\n",
    "            dropout - Amount of dropout to apply in the feed-forward network and\n",
    "                      on the input encoding\n",
    "             #source:https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial15/Vision_Transformer.html\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        # Layers/Networks\n",
    "        self.input_layer = nn.Linear(num_channels*(patch_size**2), embed_dim)\n",
    "        self.transformer = nn.Sequential(*[AttentionBlock(embed_dim, hidden_dim, num_heads, dropout=dropout) for _ in range(num_layers)])\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, num_classes)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Parameters/Embeddings\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,embed_dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1,1+num_patches,embed_dim))\n",
    "        print(num_patches)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Preprocess input\n",
    "        x = img_to_patch(x, self.patch_size)\n",
    "        B, T, _ = x.shape\n",
    "        x = self.input_layer(x)\n",
    "\n",
    "        # Add CLS token and positional encoding\n",
    "        cls_token = self.cls_token.repeat(B, 1, 1)\n",
    "        x = torch.cat([cls_token, x], dim=1)\n",
    "\n",
    "        x = x + self.pos_embedding[:,:T+1]\n",
    "\n",
    "        # Apply Transforrmer\n",
    "        x = self.dropout(x)\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # Perform classification prediction\n",
    "        cls = x[0]\n",
    "        out = self.mlp_head(cls)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a VisionTransformer object, we utilize values similar to \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.\"\n",
    "Here's a breakdown of what each parameter represents:\n",
    "\n",
    "- <code>embed_dim</code>: This parameter determines the dimensionality of the embedded feature representation of each patch in the image. In this case, it is set to 256.\n",
    "- <code>hidden_dim</code>: It represents the dimensionality of the hidden layer in the model. The hidden layer contains intermediate representations and computations. Here, it is set to 512.\n",
    "- <code>num_heads</code>: This parameter specifies the number of attention heads in the self-attention mechanism. Each head attends to different parts of the input, allowing the model to capture different types of relationships. In this case, there are 8 attention heads.\n",
    "- <code>num_layers</code>: It indicates the number of transformer layers in the VisionTransformer model. Each layer processes the input sequentially, refining the representations. Here, there are 6 transformer layers.\n",
    "- <code>patch_size</code>: It represents the size of each patch extracted from the input image. In this case, patches are square and have a size of 4x4 pixels.\n",
    "- <code>num_channels</code>: This parameter denotes the number of channels in the input image. For grayscale images, like in this case, the number of channels is 1.\n",
    "- <code>num_patches</code>: It indicates the total number of patches that make up the image. This value is calculated based on the image size and patch size. Here, there are 64 patches.\n",
    "- <code>num_classes</code>: This parameter specifies the number of classes in the classification task. In this case, the model is designed for a classification problem with 10 classes.\n",
    "- <code>dropout</code>: It represents the dropout rate, which is a regularization technique used to prevent overfitting. Dropout randomly sets a fraction of the input units to 0 during training. Here, the dropout rate is set to 0.2, meaning 20% of the input units will be zeroed out during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim= 256\n",
    "hidden_dim=512\n",
    "num_heads= 8\n",
    "num_layers=6\n",
    "patch_size=4\n",
    "num_channels=1\n",
    "num_patches=64\n",
    "num_classes=10\n",
    "dropout=0.2\n",
    "model=VisionTransformer( embed_dim= embed_dim, hidden_dim=hidden_dim, num_channels=num_channels, num_heads=num_heads, num_layers=num_layers, num_classes=num_classes, patch_size=patch_size, num_patches=num_patches, dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate scheduler is a feature provided by the PyTorch library. It allows for the adjustment of the learning rate of an optimizer at specific intervals or epochs during the training process.\n",
    "\n",
    "- <code>criterion</code>: This variable represents the loss function used for the optimization process. In our case, the nn.CrossEntropyLoss() function is used, which is commonly employed for multi-class classification tasks.\n",
    "- <code>optimizer</code>: This variable represents the optimizer used to update the model's parameters during the training process. The optim.AdamW() function is used here, which is an extension of the Adam optimizer with weight decay regularization.\n",
    "- <code>scheduler</code>: This variable represents the learning rate scheduler, which adjusts the learning rate of the optimizer during training. In the given code snippet, an ExponentialLR scheduler is used.\n",
    "\n",
    "During training, you can update the learning rate by calling the step() method of the scheduler at the end of each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=3e-4\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this method is computationally intensive, we will provide you with the model.\n",
    "\n",
    "__Even 1 epoch for training takes long time.__ You can __skip the training__ here and import the pre-trained model following the instruction below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "useful_stuff = {'training_loss': [], 'validation_accuracy': []}\n",
    "correct_old=0\n",
    "\n",
    "for epoch in tqdm(range(100)):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        model.train()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "    scheduler.step()\n",
    "    useful_stuff['training_loss'].append(running_loss)\n",
    "    correct = 0\n",
    "    model.eval()\n",
    "\n",
    "    for inputs, labels in test_loader:\n",
    "      inputs, labels = inputs.to(device), labels.to(device)\n",
    "      z = model(inputs)\n",
    "      _, yhat = torch.max(z, 1)\n",
    "      correct += (yhat == labels).sum().item()\n",
    "      if correct>correct_old:\n",
    "          correct_old=correct\n",
    "\n",
    "    useful_stuff['validation_accuracy'].append(correct_old/len(test_set))\n",
    "print('Finished Training')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Pre-trained model\n",
    "\n",
    "Training a Vision Transformer Model can be a time-consuming process. If our goal is to evaluate the performance of a fully trained and optimized model, it becomes necessary to do it for a big number of epochs.\n",
    "\n",
    "Therefore, in order to expedite the training process and avoid excessively long training times for this lab, we will simplify the procedure by providing you with the option to download the pre-trained parameters of the Vision Transformers. By utilizing these pre-trained parameters, you will be able to obtain a pre-trained model, which can be directly employed to generate images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX0CLHEN/gray_5_blocks_v2.pt\"\n",
    "await skillsnetwork.prepare(dataset_url, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pre-trained model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('gray_5_blocks_v2.pt',map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line creates an iterator **`it`** from the **`test_loader`** data loader. The iterator allows us to iterate over the test dataset in batches, enabling us to access and process the data in a sequential manner.\n",
    "\n",
    "The second line retrieves the next batch of data from the iterator using the **`next()`** function and in this case, the retrieved batch contains both images and their corresponding labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it=iter(test_loader)\n",
    "images= next(it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is to capture the activation values of specific layers in the model during the forward pass. By using the custom activation hook and the activations list, the code enables the collection of activation values for further analysis or visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_samples = 100\n",
    "\n",
    "# Create a random input tensor\n",
    "input_tensor = images[0:number_samples][0]\n",
    "input_labels = images[0:number_samples][1]\n",
    "\n",
    "# Define a list to store the activation values\n",
    "activations = []\n",
    "\n",
    "# Define a custom activation hook function\n",
    "def activation_hook(module, input, output):\n",
    "    activations.append(output.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over all the modules in the network\n",
    "for module in model.modules():\n",
    "    # Register the activation hook to the desired layers\n",
    "    if isinstance(module, nn.Linear):\n",
    "        module.register_forward_hook(activation_hook)\n",
    "        \n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **<code>activations</code>** list stored above is used to store the activation values obtained from specific layers of the model. These **<code>activations</code>** are then passed as input to the **<code>plot_patchs_lables()</code>** function for visualization purposes.\n",
    "\n",
    "The **<code>plot_patchs_lables()</code>** function creates a scatter plot where each data point represents an activation. The scatter plot assigns colors to the data points based on their corresponding class labels. Additionally, if the label parameter is set to <code>True</code>, the function annotates the scatter plot with the sequence index.\n",
    "\n",
    "During the initial epochs of training, the scatter plot may exhibit dissonance, meaning that the activation values of different classes might be mixed together, making it difficult to discern clear boundaries or separations between the classes. This is expected since the model's parameters are still being adjusted and optimized.\n",
    "\n",
    "However, as the training continues and more epochs are completed, the model learns to extract and represent distinct features that are relevant for distinguishing between different classes. This leads to a more organized scatter plot where the activation values of each class become more clustered and separated from the other classes. The distinct mapping refers to the clearer separation and grouping of the data points based on their class labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: The plotting takes around ~19 minutes per iteration, and there will be 6 iterations. Therefore, I am adding the images I have stored on my local disk. You can skip running the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_new = 0\n",
    "for i, activation in enumerate(activations):\n",
    "    if activation.shape[-1] == 256:\n",
    "        if activation.shape[0] != 1:\n",
    "            activation_new = torch.permute(activation, (1, 0, 2))\n",
    "        else:\n",
    "            activation_new = activation\n",
    "\n",
    "    if i % 5 == 0:\n",
    "        print(i)\n",
    "        plot_patchs_lables(activation_new, input_labels, label=True, p_show=0)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Iteration 1 | Iteration 2 |\n",
    "|-------------|-------------|\n",
    "| ![Iteration 1](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX0CLHEN/first.png) | ![Iteration 2](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX0CLHEN/second.png) |\n",
    "\n",
    "| Iteration 3 | Iteration 4 |\n",
    "|-------------|-------------|\n",
    "| ![Iteration 3](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX0CLHEN/third.png) | ![Iteration 4](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX0CLHEN/forth.png) |\n",
    "\n",
    "| Iteration 5 | Iteration 6 |\n",
    "|-------------|-------------|\n",
    "| ![Iteration 5](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX0CLHEN/fifth.png) | ![Iteration 6](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX0CLHEN/sixth.png) |\n",
    "\n",
    "| Iteration 7 |\n",
    "|-------------|\n",
    "| ![Iteration 7](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX0CLHEN/seventh.png) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the iterator is used to get images and labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it=iter(test_loader)\n",
    "images, labels= next(it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output for the model\n",
    "\n",
    "Lets classify the image using the output of the model\n",
    "\n",
    "#### Softmax\n",
    "Suppose we have a model that predicts the likelihood of three classes: Cat, Dog, and Bird. The model outputs raw scores for each class, indicating the model's confidence. Let's say the model's output raw scores for an input example are the input neural network:\n",
    "\n",
    "$$\\text{Raw scores} = [3.2, 2.1, 1.5]$$\n",
    "\n",
    "To calculate the softmax probabilities, we apply the softmax function to these raw scores:\n",
    "\n",
    "$$\n",
    "\\text{softmax}([3.2, 2.1, 1.5]) = \\left[ \\frac{e^{3.2}}{e^{3.2} + e^{2.1} + e^{1.5}}, \\frac{e^{2.1}}{e^{3.2} + e^{2.1} + e^{1.5}}, \\frac{e^{1.5}}{e^{3.2} + e^{2.1} + e^{1.5}} \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "Now, let's calculate the softmax values:\n",
    "\n",
    "$$\\text{softmax}([3.2, 2.1, 1.5]) ≈ [0.533, 0.278, 0.189]$$\n",
    "\n",
    "**In our case, the same principle is applied to the MNIST dataset, where each class from 0 to 9 is assigned a probability value. The label with the highest probability value is selected as the predicted label.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Argmax\n",
    "\n",
    "The **<code>argmax()</code>** function is a mathematical function that returns the index of the maximum value in a tensor or array along a specified axis. In the context of the code snippet provided, **<code>argmax(dim=1)</code>** is used to find the index of the class with the highest probability for each image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only 3 images are selected\n",
    "images = images[:3]\n",
    "labels = labels[:3]\n",
    "\n",
    "p_yx = model(images)\n",
    "\n",
    "# Get predicted probabilities\n",
    "probabilities = F.softmax(p_yx, dim=1)\n",
    "\n",
    "# Find predicted labels\n",
    "predicted_labels = probabilities.argmax(dim=1)\n",
    "\n",
    "# Compare predicted labels with actual labels and display the images\n",
    "for i in range(len(images)):\n",
    "    # Convert the image tensor to a numpy array\n",
    "    image = images[i].squeeze().numpy()\n",
    "\n",
    "    # Display the image in grayscale\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Predicted label:\", predicted_labels[i])\n",
    "    print(\"Actual label:\", labels[i])\n",
    "    print(\"Probabilities:\")\n",
    "\n",
    "    # Print probabilities for each class\n",
    "    for j in range(len(probabilities[i])):\n",
    "        class_name = f\"Class {j}\"\n",
    "        probability = probabilities[i][j].item()\n",
    "        print(f\"{class_name}: {probability}\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations! You have completed the lab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Joseph Santarcangelo](https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX0CLHEN3536-2023-01-01) has a Ph.D. in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contributor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Roodra Kanwar](https://www.linkedin.com/in/roodrakanwar/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkGuidedProjectsIBMSkillsNetworkGPXX0CLHEN3536-2023-01-01) is completing his MS in CS specializing in big data from Simon Fraser University. He has previous experience working with machine learning and as a data engineer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
    "|-|-|-|-|\n",
    "|2023-07-26|0.1|Joseph|Created Lab Template|\n",
    "|2023-07-28|0.1|Roodra|Edited & Reviewed Lab|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2023 IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
